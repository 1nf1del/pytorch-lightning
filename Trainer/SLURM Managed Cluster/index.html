<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>SLURM Managed Cluster - Pytorch lightning Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "SLURM Managed Cluster";
    var mkdocs_page_input_path = "Trainer/SLURM Managed Cluster.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Pytorch lightning Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">PYTORCH-LIGHTNING DOCUMENTATION</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../Examples/">Examples</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">LightningModule</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../LightningModule/RequiredTrainerInterface/">Lightning Module interface</a>
                </li>
                <li class="">
                    
    <a class="" href="../../LightningModule/methods/">Methods</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Trainer</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../">Trainer</a>
                </li>
                <li class="">
                    
    <a class="" href="../Checkpointing/">Checkpointing</a>
                </li>
                <li class="">
                    
    <a class="" href="../Distributed training/">Distributed training</a>
                </li>
                <li class="">
                    
    <a class="" href="../Logging/">Logging</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">SLURM Managed Cluster</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#running-grid-search-on-a-cluster">Running grid search on a cluster</a></li>
    

    <li class="toctree-l3"><a href="#walltime-auto-resubmit">Walltime auto-resubmit</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../Training Loop/">Training Loop</a>
                </li>
                <li class="">
                    
    <a class="" href="../Validation loop/">Validation loop</a>
                </li>
                <li class="">
                    
    <a class="" href="../debugging/">Debugging</a>
                </li>
                <li class="">
                    
    <a class="" href="../hooks/">Hooks</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Pytorch lightning Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Trainer &raquo;</li>
        
      
    
    <li>SLURM Managed Cluster</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/williamFalcon/pytorch-lightning/edit/master/docs/Trainer/SLURM Managed Cluster.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>Lightning supports model training on a cluster managed by SLURM in the following cases:    </p>
<ol>
<li>Training on single or multi-cpus only.</li>
<li>Training on single or multi-gpus on the same node.</li>
<li>Coming SOON: Training across multiple nodes.</li>
</ol>
<hr />
<h4 id="running-grid-search-on-a-cluster">Running grid search on a cluster</h4>
<p>To use lightning to run a hyperparameter search (grid-search or random-search) on a cluster do 4 things:   </p>
<p>(1). Define the parameters for the grid search    </p>
<pre><code class="python">from test_tube import HyperOptArgumentParser

# subclass of argparse
parser = HyperOptArgumentParser(strategy='random_search')
parser.add_argument('--learning_rate', default=0.002, type=float, help='the learning rate')

# let's enable optimizing over the number of layers in the network
parser.opt_list('--nb_layers', default=2, type=int, tunable=True, options=[2, 4, 8])

hparams = parser.parse_args()    
</code></pre>

<p>(2). Define the cluster options in the <a href="https://williamfalcon.github.io/test-tube/hpc/SlurmCluster/">SlurmCluster object</a> (over 5 nodes and 8 gpus)    </p>
<pre><code class="python">from test_tube.hpc import SlurmCluster

# hyperparameters is a test-tube hyper params object
# see https://williamfalcon.github.io/test-tube/hyperparameter_optimization/HyperOptArgumentParser/
hyperparams = args.parse()

# init cluster
cluster = SlurmCluster(
    hyperparam_optimizer=hyperparams,
    log_path='/path/to/log/results/to',
    python_cmd='python3'
)

# let the cluster know where to email for a change in job status (ie: complete, fail, etc...)
cluster.notify_job_status(email='some@email.com', on_done=True, on_fail=True)

# set the job options. In this instance, we'll run 20 different models
# each with its own set of hyperparameters giving each one 1 GPU (ie: taking up 20 GPUs)
cluster.per_experiment_nb_gpus = 8
cluster.per_experiment_nb_nodes = 5

# we'll request 10GB of memory per node
cluster.memory_mb_per_node = 10000

# set a walltime of 10 minues
cluster.job_time = '10:00'
</code></pre>

<p>(3). Give trainer the cluster_manager in your main function:    </p>
<pre><code class="python">from pytorch_lightning import Trainer

def train_fx(trial_hparams, cluster_manager, _):
    # hparams has a specific set of hyperparams

    my_model = MyLightningModel()

    # give the trainer the cluster object
    trainer = Trainer(cluster=cluster_manager)
    trainer.fit(my_model)

</code></pre>

<p>(4). Start the grid search     </p>
<pre><code class="python"># run the models on the cluster
cluster.optimize_parallel_cluster_gpu(
    train_fx, 
    nb_trials=20, 
    job_name='my_grid_search_exp_name', 
    job_display_name='my_exp')
</code></pre>

<p>That's it! The SlurmCluster object will automatically checkpoint the lightning model and resubmit if it runs into the walltime!</p>
<hr />
<h4 id="walltime-auto-resubmit">Walltime auto-resubmit</h4>
<p>Lightning automatically resubmits jobs when they reach the walltime. You get this behavior for free if you give lightning
a slurm cluster object.</p>
<pre><code class="python">def my_main_fx(hparams, slurm_manager, _):
    trainer = Trainer(cluster=slurm_manager)
</code></pre>

<p>(See the grid search example above for cluster configuration).
With this feature lightning will:    </p>
<ol>
<li>automatically checkpoint the model</li>
<li>checkpoint the trainer session</li>
<li>resubmit a continuation job.</li>
<li>load the checkpoint and trainer session in the new model</li>
</ol>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Training Loop/" class="btn btn-neutral float-right" title="Training Loop">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../Logging/" class="btn btn-neutral" title="Logging"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/williamFalcon/pytorch-lightning/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../Logging/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../Training Loop/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
