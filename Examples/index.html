<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Examples - Pytorch lightning Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Examples";
    var mkdocs_page_input_path = "Examples.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Pytorch lightning Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">PYTORCH-LIGHTNING DOCUMENTATION</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Examples</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#template-model-definition">Template model definition</a></li>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">LightningModule</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../LightningModule/RequiredTrainerInterface/">Lightning Module interface</a>
                </li>
                <li class="">
                    
    <a class="" href="../LightningModule/methods/">Methods</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Trainer</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../Trainer/">Trainer</a>
                </li>
                <li class="">
                    
    <a class="" href="../Trainer/Checkpointing/">Checkpointing</a>
                </li>
                <li class="">
                    
    <a class="" href="../Trainer/Distributed training/">Distributed training</a>
                </li>
                <li class="">
                    
    <a class="" href="../Trainer/Logging/">Logging</a>
                </li>
                <li class="">
                    
    <a class="" href="../Trainer/SLURM Managed Cluster/">SLURM Managed Cluster</a>
                </li>
                <li class="">
                    
    <a class="" href="../Trainer/Training Loop/">Training Loop</a>
                </li>
                <li class="">
                    
    <a class="" href="../Trainer/Validation loop/">Validation loop</a>
                </li>
                <li class="">
                    
    <a class="" href="../Trainer/debugging/">Debugging</a>
                </li>
                <li class="">
                    
    <a class="" href="../Trainer/hooks/">Hooks</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Pytorch lightning Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Examples</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/williamFalcon/pytorch-lightning/edit/master/docs/Examples.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h4 id="template-model-definition">Template model definition</h4>
<p>In 99% of cases you want to just copy this template to start a new lightningModule and change the core of what your model is actually trying to do.</p>
<pre><code class="python">import os
from collections import OrderedDict
import torch.nn as nn
from torchvision.datasets import MNIST
import torchvision.transforms as transforms
import torch
import torch.nn.functional as F
from test_tube import HyperOptArgumentParser
from torch import optim

from pytorch_lightning.root_module.root_module import LightningModule


class LightningTemplateModel(LightningModule):
    &quot;&quot;&quot;
    Sample model to show how to define a template
    &quot;&quot;&quot;

    def __init__(self, hparams):
        &quot;&quot;&quot;
        Pass in parsed HyperOptArgumentParser to the model
        :param hparams:
        &quot;&quot;&quot;
        # init superclass
        super(LightningTemplateModel, self).__init__(hparams)

        self.batch_size = hparams.batch_size

        # build model
        self.__build_model()

    # ---------------------
    # MODEL SETUP
    # ---------------------
    def __build_model(self):
        &quot;&quot;&quot;
        Layout model
        :return:
        &quot;&quot;&quot;
        self.c_d1 = nn.Linear(in_features=self.hparams.in_features, out_features=self.hparams.hidden_dim)
        self.c_d1_bn = nn.BatchNorm1d(self.hparams.hidden_dim)
        self.c_d1_drop = nn.Dropout(self.hparams.drop_prob)

        self.c_d2 = nn.Linear(in_features=self.hparams.hidden_dim, out_features=self.hparams.out_features)

    # ---------------------
    # TRAINING
    # ---------------------
    def forward(self, x):
        &quot;&quot;&quot;
        No special modification required for lightning, define as you normally would
        :param x:
        :return:
        &quot;&quot;&quot;

        x = self.c_d1(x)
        x = torch.tanh(x)
        x = self.c_d1_bn(x)
        x = self.c_d1_drop(x)

        x = self.c_d2(x)
        logits = F.log_softmax(x, dim=1)

        return logits

    def loss(self, labels, logits):
        nll = F.nll_loss(logits, labels)
        return nll

    def training_step(self, data_batch, batch_i):
        &quot;&quot;&quot;
        Lightning calls this inside the training loop
        :param data_batch:
        :return:
        &quot;&quot;&quot;
        # forward pass
        x, y = data_batch
        x = x.view(x.size(0), -1)
        y_hat = self.forward(x)

        # calculate loss
        loss_val = self.loss(y, y_hat)

        output = OrderedDict({
            'loss': loss_val,
            'tqdm_metrics': {}
        })
        return output

    def validation_step(self, data_batch, batch_i):
        &quot;&quot;&quot;
        Lightning calls this inside the validation loop
        :param data_batch:
        :return:
        &quot;&quot;&quot;
        x, y = data_batch
        x = x.view(x.size(0), -1)
        y_hat = self.forward(x)

        loss_val = self.loss(y, y_hat)

        # acc
        labels_hat = torch.argmax(y_hat, dim=1)
        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)

        output = OrderedDict({
            'val_loss': loss_val,
            'val_acc': torch.tensor(val_acc),
        })
        return output

    def validation_end(self, outputs):
        &quot;&quot;&quot;
        Called at the end of validation to aggregate outputs
        :param outputs: list of individual outputs of each validation step
        :return:
        &quot;&quot;&quot;
        val_loss_mean = 0
        val_acc_mean = 0
        for output in outputs:
            val_loss_mean += output['val_loss']
            val_acc_mean += output['val_acc']

        val_loss_mean /= len(outputs)
        val_acc_mean /= len(outputs)
        tqdm_dic = {'val_loss': val_loss_mean.item(), 'val_acc': val_acc_mean.item()}
        return tqdm_dic

    def update_tng_log_metrics(self, logs):
        return logs

    # ---------------------
    # MODEL SAVING
    # ---------------------
    def get_save_dict(self):
        checkpoint = {'state_dict': self.state_dict()}
        return checkpoint

    def load_model_specific(self, checkpoint):
        self.load_state_dict(checkpoint['state_dict'])
        pass

    # ---------------------
    # TRAINING SETUP
    # ---------------------
    def configure_optimizers(self):
        &quot;&quot;&quot;
        return whatever optimizers we want here
        :return: list of optimizers
        &quot;&quot;&quot;
        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)
        return [optimizer]

    def __dataloader(self, train):
        # init data generators
        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])

        dataset = MNIST(root=self.hparams.data_root, train=train, transform=transform, download=True)

        loader = torch.utils.data.DataLoader(
            dataset=dataset,
            batch_size=self.hparams.batch_size,
            shuffle=True
        )

        return loader

    @property
    def tng_dataloader(self):
        if self._tng_dataloader is None:
            try:
                self._tng_dataloader = self.__dataloader(train=True)
            except Exception as e:
                print(e)
                raise e
        return self._tng_dataloader

    @property
    def val_dataloader(self):
        if self._val_dataloader is None:
            try:
                self._val_dataloader = self.__dataloader(train=False)
            except Exception as e:
                print(e)
                raise e
        return self._val_dataloader

    @property
    def test_dataloader(self):
        if self._test_dataloader is None:
            try:
                self._test_dataloader = self.__dataloader(train=False)
            except Exception as e:
                print(e)
                raise e
        return self._test_dataloader

    @staticmethod
    def add_model_specific_args(parent_parser, root_dir):
        &quot;&quot;&quot;
        Parameters you define here will be available to your model through self.hparams
        :param parent_parser:
        :param root_dir:
        :return:
        &quot;&quot;&quot;
        parser = HyperOptArgumentParser(strategy=parent_parser.strategy, parents=[parent_parser])

        # param overwrites
        # parser.set_defaults(gradient_clip=5.0)

        # network params
        parser.opt_list('--drop_prob', default=0.2, options=[0.2, 0.5], type=float, tunable=False)
        parser.add_argument('--in_features', default=28*28)
        parser.add_argument('--out_features', default=10)
        parser.add_argument('--hidden_dim', default=50000) # use 500 for CPU, 50000 for GPU to see speed difference

        # data
        parser.add_argument('--data_root', default=os.path.join(root_dir, 'mnist'), type=str)

        # training params (opt)
        parser.opt_list('--learning_rate', default=0.001, type=float, options=[0.0001, 0.0005, 0.001, 0.005],
                        tunable=False)
        parser.opt_list('--batch_size', default=256, type=int, options=[32, 64, 128, 256], tunable=False)
        parser.opt_list('--optimizer_name', default='adam', type=str, options=['adam'], tunable=False)
        return parser

</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../LightningModule/RequiredTrainerInterface/" class="btn btn-neutral float-right" title="Lightning Module interface">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href=".." class="btn btn-neutral" title="PYTORCH-LIGHTNING DOCUMENTATION"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/williamFalcon/pytorch-lightning/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href=".." style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../LightningModule/RequiredTrainerInterface/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
